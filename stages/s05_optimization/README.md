# Stage 05: Optimization Fundamentals

**Duration**: 3-4 days
**Prerequisites**: s02, s03, s04
**Difficulty**: ⭐⭐⭐☆☆

## What You'll Learn

Optimization is how we train ML models. You'll implement:
- Gradient descent and variants (momentum)
- Modern optimizers (AdaGrad, RMSProp, Adam)
- Learning rate schedules

## Why This Matters

- **Training neural networks**: All use these optimizers
- **Understanding convergence**: Why some optimizers work better
- **Hyperparameter tuning**: Choosing learning rates, momentum, etc.

## Next Stage

**s06: Linear Regression from Scratch** - Apply optimization to real ML!
