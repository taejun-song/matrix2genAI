# Stage 6: Linear Regression - Building Blocks

## Overview

Build a complete linear regression system from simple, composable functions. No classes, no frameworks - just pure functions that you'll compose into a working ML system.

**The Big Idea:** Linear regression finds the "best fit line" through data points. It's the foundation of all machine learning - once you understand this, everything else is just variations!

## Learning Philosophy

You will implement **small building blocks** (1-3 lines each) that compose into complete ML algorithms:
- Gradient descent training loop
- Visualization and evaluation
- Comparison with closed-form solution

**Time:** 2-3 hours
**Difficulty:** â­â­â­

## Getting Started

### Setup

```bash
# Navigate to this stage
cd stages/s06_linear_regression

# Run tests (using uv - recommended)
uv run pytest tests/ -v

# Or activate venv first
source .venv/bin/activate  # On Unix/macOS
# .venv\Scripts\activate   # On Windows
pytest tests/ -v
```

### Files You'll Edit

- `starter/primitives.py` - Implement the 8 building blocks here
- Tests are in `tests/test_primitives.py` - run them to verify your work

### Workflow

1. Read this README to understand concepts
2. Implement sub-functions in `starter/primitives.py` (follow TODOs)
3. Run `uv run pytest tests/test_primitives.py::TestClassName -v` to test each function
4. Once all tests pass, compose functions into training loops
5. Experiment and visualize!

---

## Conceptual Understanding

### What is Linear Regression?

**Goal:** Given data points, find the line that best fits them.

```
Example: Predict house price from size

Data points:          Best fit line:
  Size | Price         Price = 150Â·Size + 50000
  1000 | 200k
  1500 | 275k          y
  2000 | 350k          |     â•± (prediction line)
                       |   â•±
Question:              | â•±
  Size=1800 â†’ Price=?  |______ x (size)

Answer: 150Â·1800 + 50000 = $320k
```

**The Math:** Å· = wÂ·x + b
- **w** (weight/slope): How much price increases per sq ft
- **b** (bias/intercept): Base price (where line crosses y-axis)
- **Å·**: Predicted value

### Why Do We Need a Loss Function?

**Problem:** Infinite possible lines! Which one is "best"?

```
Three possible lines:
  Line A: Å· = 100x + 70000   (too flat, underestimates)
  Line B: Å· = 150x + 50000   (just right!)
  Line C: Å· = 200x + 30000   (too steep, overestimates)

How do we measure "goodness of fit"?
```

**Solution:** Mean Squared Error (MSE)
```
MSE = (1/n) Î£ (actual - predicted)Â²

Line A: MSE = 5000  (bad)
Line B: MSE = 100   (good!)
Line C: MSE = 3000  (bad)

Why square the errors?
  â€¢ Penalizes big mistakes more (10Â² = 100 vs 2Â·10 = 20)
  â€¢ No negative errors canceling positive ones
  â€¢ Smooth, differentiable (can use calculus!)
```

### Gradient Descent: The Optimization Algorithm

**Intuition:** Imagine you're blindfolded on a hill, trying to reach the valley (minimum MSE).

```
Strategy:
  1. Feel which direction is downhill (compute gradient)
  2. Take a small step in that direction (update weights)
  3. Repeat until you reach the valley (convergence)

In math:
  gradient = âˆ‚MSE/âˆ‚w = how much MSE changes when we change w

  Update rule:
    w_new = w_old - learning_rate Â· gradient

  Example:
    Current: w=100, MSE=5000
    Gradient: âˆ‚MSE/âˆ‚w = +500 (increasing w increases MSE)
    Update: w = 100 - 0.01Â·500 = 95 (move opposite direction!)
```

### The RÂ² Score: Measuring Success

**Question:** Is MSE=100 good or bad? Depends on scale!

**RÂ²:** Compares your model to the simplest baseline (predicting mean)

```
RÂ² = 1 - (your_model_error / baseline_error)

Interpretation:
  RÂ² = 1.0  â†’ Perfect! Every prediction exactly right
  RÂ² = 0.9  â†’ Explains 90% of variance (very good)
  RÂ² = 0.5  â†’ Explains 50% of variance (okay)
  RÂ² = 0.0  â†’ As good as predicting average (bad)
  RÂ² < 0.0  â†’ Worse than predicting average (terrible!)

Example:
  Baseline (predict mean): always predict $250k
    Error on test set: SS_tot = 10,000

  Your model: Å· = 150x + 50000
    Error on test set: SS_res = 1,000

  RÂ² = 1 - 1000/10000 = 0.9  âœ“ Great!
```

### Normal Equations: The Shortcut

**Two ways to find the best line:**

```
1. Gradient Descent (iterative):
   âœ“ Works for any model
   âœ“ Memory efficient
   âœ— Needs hyperparameters (learning rate)
   âœ— May take many iterations

2. Normal Equations (closed-form):
   âœ“ Direct solution (one step!)
   âœ“ Always finds optimal answer
   âœ— Only works for linear regression
   âœ— Slow for large datasets (matrix inversion)

Formula: w = (Xáµ€X)â»Â¹Xáµ€y

When to use:
  â€¢ Small dataset (< 10,000 samples): Normal equations
  â€¢ Large dataset (> 10,000 samples): Gradient descent
```

## What You'll Build

### Core Building Blocks (8 functions)

1. `predict(X, weights, bias)` - Compute predictions
2. `mse_loss(y_true, y_pred)` - Mean squared error
3. `mse_gradient(X, y_true, y_pred)` - Loss gradients
4. `r2_score(y_true, y_pred)` - Coefficient of determination
5. `normal_equation(X, y)` - Closed-form solution
6. `standardize(X)` - Z-score normalization
7. `train_test_split(X, y, test_size)` - Data splitting
8. `polynomial_features(X, degree)` - Feature expansion

### What You'll Compose

Using these blocks, you'll write:
- A gradient descent training loop
- Model evaluation pipeline
- Experiments comparing different approaches

## Mathematical Background

### Linear Model
```
Å· = Xw + b

where:
  X âˆˆ â„â¿Ë£áµˆ  (n samples, d features)
  w âˆˆ â„áµˆ    (weights)
  b âˆˆ â„     (bias)
  Å· âˆˆ â„â¿    (predictions)
```

### Mean Squared Error (MSE)
```
L(w,b) = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

### Gradients
```
âˆ‚L/âˆ‚w = (2/n) Î£áµ¢ (Å·áµ¢ - yáµ¢) Â· xáµ¢ = (2/n) Xáµ€(Å· - y)
âˆ‚L/âˆ‚b = (2/n) Î£áµ¢ (Å·áµ¢ - yáµ¢)
```

### Gradient Descent Update
```
w â† w - Î± Â· âˆ‚L/âˆ‚w
b â† b - Î± Â· âˆ‚L/âˆ‚b
```

### Normal Equations (Closed-Form)
```
[b]   = ([1â‚™ | X]áµ€ [1â‚™ | X])â»Â¹ [1â‚™ | X]áµ€ y
[w]
```

### RÂ² Score
```
RÂ² = 1 - (SS_res / SS_tot)

where:
  SS_res = Î£(y - Å·)Â²  (residual sum of squares)
  SS_tot = Î£(y - È³)Â²  (total sum of squares)
```

## Implementation Guide

### Step 1: Core Primitives (30 min)

Implement the 8 building blocks. Each is simple:

**Example: `predict`**
```python
def predict(X, weights, bias):
    return X @ weights + bias  # That's it!
```

**Example: `mse_loss`**
```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)  # One line!
```

### Step 2: Test Each Block (30 min)

Run tests to verify each function works:
```bash
# If using uv (recommended):
uv run pytest tests/test_primitives.py -v

# Or if venv is activated:
pytest tests/test_primitives.py -v
```

Each function is tested independently with clear inputs/outputs.

### Step 3: Compose Training Loop (45 min)

Write your own gradient descent loop:

```python
# Initialize
weights = np.zeros(n_features)
bias = 0.0
learning_rate = 0.01

# Train
for epoch in range(1000):
    # Forward pass
    y_pred = predict(X_train, weights, bias)

    # Compute loss
    loss = mse_loss(y_train, y_pred)

    # Compute gradients
    grad_w, grad_b = mse_gradient(X_train, y_train, y_pred)

    # Update parameters
    weights -= learning_rate * grad_w
    bias -= learning_rate * grad_b

    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss = {loss:.4f}")
```

### Step 4: Evaluate and Visualize (30 min)

Use your building blocks:

```python
# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Standardize
X_train_scaled, mean, std = standardize(X_train)
X_test_scaled = (X_test - mean) / std

# Train with gradient descent
# ... (your training loop)

# Compare with normal equation
weights_ne, bias_ne = normal_equation(X_train_scaled, y_train)

# Evaluate both
y_pred_gd = predict(X_test_scaled, weights, bias)
y_pred_ne = predict(X_test_scaled, weights_ne, bias_ne)

print(f"GD RÂ²: {r2_score(y_test, y_pred_gd):.4f}")
print(f"NE RÂ²: {r2_score(y_test, y_pred_ne):.4f}")
```

## Key Concepts

### Why Standardization Matters

Without standardization:
```
Feature 1: [1, 2, 3]          â†’ gradient scale ~1
Feature 2: [1000, 2000, 3000] â†’ gradient scale ~1000
```

Gradient descent struggles! After standardization:
```
Feature 1: [-1, 0, 1]  â†’ gradient scale ~1
Feature 2: [-1, 0, 1]  â†’ gradient scale ~1
```

Much better convergence!

### Gradient Descent vs Normal Equations

**Gradient Descent:**
- âœ… Works with huge datasets
- âœ… Can do online learning
- âœ… Generalizes to non-linear models
- âŒ Requires tuning learning rate
- âŒ Needs multiple iterations

**Normal Equations:**
- âœ… Exact solution in one step
- âœ… No hyperparameters
- âŒ Requires matrix inversion O(dÂ³)
- âŒ Needs full dataset in memory
- âŒ Doesn't work for non-linear models

### RÂ² Interpretation

- **RÂ² = 1.0**: Perfect predictions
- **RÂ² = 0.0**: Model is as good as predicting the mean
- **RÂ² < 0.0**: Model is worse than predicting the mean!

## Common Pitfalls

### 1. Forgetting to Standardize

```python
# âŒ Bad: Different scales
X_train = np.array([[1, 1000], [2, 2000]])
# Gradient descent will struggle!

# âœ… Good: Same scale
X_train_scaled, mean, std = standardize(X_train)
# [[âˆ’1, âˆ’1], [1, 1]] - much better!
```

### 2. Not Applying Same Transform to Test Data

```python
# âŒ Bad: Different transforms
X_train_scaled, mean, std = standardize(X_train)
X_test_scaled, _, _ = standardize(X_test)  # Different mean/std!

# âœ… Good: Use training statistics
X_train_scaled, mean, std = standardize(X_train)
X_test_scaled = (X_test - mean) / std  # Same transform
```

### 3. Learning Rate Too Large

```python
learning_rate = 1.0  # Too large!
# Loss will oscillate or diverge: 100 â†’ 50 â†’ 200 â†’ 1000 â†’ NaN

learning_rate = 0.01  # Better
# Loss decreases smoothly: 100 â†’ 50 â†’ 25 â†’ 12 â†’ 6 â†’ 3
```

### 4. Not Checking Loss Decreases

```python
# Always monitor loss!
for epoch in range(1000):
    loss = mse_loss(y_train, y_pred)

    # Sanity check
    if epoch > 0 and loss > prev_loss:
        print(f"Warning: Loss increased! {prev_loss:.4f} â†’ {loss:.4f}")
        print("Learning rate might be too large!")

    prev_loss = loss
```

## Experiments to Try

Once your building blocks work, experiment:

### 1. Learning Rate Comparison
```python
for lr in [0.001, 0.01, 0.1, 1.0]:
    # Train model with this learning rate
    # Plot loss curves
    # Which converges fastest?
```

### 2. Polynomial Features
```python
# Try different polynomial degrees
for degree in [1, 2, 3, 5]:
    X_poly = polynomial_features(X, degree)
    # Does higher degree always improve RÂ²?
    # What about test RÂ² vs train RÂ²?
```

### 3. Dataset Size Effect
```python
for n in [10, 50, 100, 500, 1000]:
    X_subset = X[:n]
    y_subset = y[:n]
    # How does RÂ² change with more data?
    # When does normal equation become slow?
```

## Debugging Guide

### Predictions are all wrong
```python
# Check your prediction function
X = np.array([[1, 2], [3, 4]])
w = np.array([1, 1])
b = 0

y_pred = predict(X, w, b)
print(y_pred)  # Should be [3, 7]

# If not, check matrix shapes:
print(f"X shape: {X.shape}")      # (2, 2)
print(f"w shape: {w.shape}")      # (2,)
print(f"X @ w shape: {(X @ w).shape}")  # (2,)
```

### Loss not decreasing
```python
# 1. Check gradient computation
from stages.s03_calculus.starter.numerical_diff import gradient_check

def loss_fn(w):
    y_pred = predict(X, w, bias)
    return mse_loss(y, y_pred)

# Compare analytical vs numerical gradients
grad_analytical, _ = mse_gradient(X, y, y_pred)
is_correct = gradient_check(loss_fn, weights, grad_analytical)
print(f"Gradient correct: {is_correct}")

# 2. Try smaller learning rate
learning_rate /= 10

# 3. Check if standardization is applied
print(f"X mean: {X.mean(axis=0)}")  # Should be ~0 if standardized
print(f"X std: {X.std(axis=0)}")    # Should be ~1 if standardized
```

### Normal equation fails
```python
# Check for singular matrix (correlated features)
X_aug = np.column_stack([np.ones(len(X)), X])
rank = np.linalg.matrix_rank(X_aug)
print(f"Matrix rank: {rank}, Expected: {X_aug.shape[1]}")

if rank < X_aug.shape[1]:
    print("Matrix is singular! Features might be correlated.")
    # Use np.linalg.lstsq instead of solve
```

## Testing Your Implementation

```bash
# Test individual building blocks
pytest tests/test_primitives.py -v

# Test gradient correctness
pytest tests/test_gradients.py -v

# Test full composition
pytest tests/test_composition.py -v

# Grade your work
python scripts/grade.py s06_linear_regression
```

## Real-World Applications

Linear regression is everywhere:

- **Housing Prices**: Predict price from sqft, bedrooms, location
- **Sales Forecasting**: Predict revenue from advertising spend
- **Medical**: Predict patient outcomes from measurements
- **Finance**: Model stock returns from economic indicators
- **Science**: Discover relationships between variables

Even in 2024, linear regression is often the **first thing** ML engineers try because:
- Fast to train
- Easy to interpret (feature coefficients)
- Strong baseline for comparison
- Works surprisingly well for many problems

## What's Next

After mastering these building blocks:

**s07: Logistic Regression** - Add sigmoid and cross-entropy loss
**s08: Feature Engineering** - PCA, encoding, imputation
**s09: Regularization** - L1/L2 penalties (just add to gradient!)
**s12: Neural Networks** - Stack linear layers with non-linearities

The pattern stays the same:
1. Simple building block functions
2. Test each independently
3. Compose into complete systems

## Success Criteria

You understand this stage when you can:

- âœ… Explain what each of the 8 functions does
- âœ… Write a training loop from scratch
- âœ… Debug why loss isn't decreasing
- âœ… Explain when to use GD vs normal equations
- âœ… Add new features (e.g., momentum) by modifying the loop

**Target: 90%+ test passing rate**

Good luck! Remember: each function is simple. The power comes from composition. ğŸš€
